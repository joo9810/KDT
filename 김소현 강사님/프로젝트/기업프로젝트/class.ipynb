{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class NERTrainer:\n",
    "    def __init__(self, model_name='klue/bert-base', num_labels=3, label_names=['그 외', '매장명', '음식명']):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def make_new_dataset(self, file_path):\n",
    "        dataset = load_dataset('json', data_files=file_path)\n",
    "\n",
    "        tokens_list = []\n",
    "        ner_tags_list = []\n",
    "        input_ids_list = []\n",
    "        token_type_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        file_name = file_path.split('.')[0] # 'train' 글자 추출 용도\n",
    "\n",
    "        for i in range(len(dataset[file_name])):\n",
    "            tokens = dataset[file_name][i]['tokens'] # dataset['train'][0]['tokens'] 형식\n",
    "            tokens_list.append(tokens)\n",
    "\n",
    "            ner_tags = dataset[file_name][i]['ner_tags']\n",
    "            ner_tags_list.append(ner_tags)\n",
    "\n",
    "            tokenized_inputs = self.tokenizer(tokens, truncation=True, is_split_into_words=True)\n",
    "            # truncation=True : 시퀀스가 모델의 최대 길이를 초과할 경우, 초과 부분을 잘라냄\n",
    "            input_ids = tokenized_inputs['input_ids']\n",
    "            input_ids_list.append(input_ids)\n",
    "\n",
    "            token_type_ids = tokenized_inputs['token_type_ids']\n",
    "            token_type_ids_list.append(token_type_ids)\n",
    "\n",
    "            attention_mask = tokenized_inputs['attention_mask']\n",
    "            attention_mask_list.append(attention_mask)\n",
    "\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "            aligned_labels = [ner_tags[word_id] if word_id is not None else -100 for word_id in word_ids]\n",
    "            labels_list.append(aligned_labels)\n",
    "\n",
    "        data_dict = {\n",
    "            'tokens' : tokens_list,\n",
    "            'ner_tags' : ner_tags_list,\n",
    "            'input_ids' : input_ids_list,\n",
    "            'token_type_ids' : token_type_ids_list,\n",
    "            'attention_mask' : attention_mask_list,\n",
    "            'labels' : labels_list\n",
    "        }\n",
    "\n",
    "        new_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "        return new_dataset\n",
    "    \n",
    "    def split_training(self, new_dataset, test_size=0.2, lr=2e-5, batch_size=1, epoch=3, save_model=False):\n",
    "        split_dataset = new_dataset.train_test_split(test_size=test_size)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",                 # 모델 학습 결과를 저장할 경로\n",
    "            eval_strategy=\"epoch\",                  # 각 epoch 이후에 평가\n",
    "            learning_rate=lr,                     # 학습률\n",
    "            per_device_train_batch_size=batch_size,          # 학습 시 배치 크기\n",
    "            per_device_eval_batch_size=batch_size,           # 평가 시 배치 크기\n",
    "            num_train_epochs=epoch,                     # 총 학습 epoch 수\n",
    "            weight_decay=0.01,                      # 가중치 감소율 (과적합 방지)\n",
    "            # seed=1                                  # 시드값\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,                       # 훈련할 모델\n",
    "            args=training_args,                     # 학습 파라미터\n",
    "            train_dataset=split_dataset[\"train\"],   # 훈련 데이터셋\n",
    "            eval_dataset=split_dataset[\"test\"],     # 평가 데이터셋\n",
    "        )\n",
    "\n",
    "        self.trainer.train()\n",
    "\n",
    "        if save_model == True:\n",
    "            self.model.save_pretrained(\"./path_to_save_model\")\n",
    "            self.tokenizer.save_pretrained(\"./path_to_save_tokenizer\")\n",
    "\n",
    "        return split_dataset\n",
    "    \n",
    "    def evaluate(self, split_dataset):\n",
    "        predictions, label_ids, metrics = self.trainer.predict(split_dataset['train'])\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "        pred_list, target_list = [], []\n",
    "        for pred, label in zip(preds, label_ids):\n",
    "            pred_list.extend(pred)\n",
    "            target_list.extend([0 if i==-100 else i for i in label])\n",
    "\n",
    "        report = classification_report(target_list, pred_list, target_names=self.label_names)\n",
    "        return report\n",
    "    \n",
    "    def pred_object(self, test_dataset):\n",
    "        predictions, label_ids, metrics = self.trainer.predict(test_dataset)\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "        tokens = test_dataset['tokens']\n",
    "        tokenized_inputs = self.tokenizer(tokens, is_split_into_words=True, truncation=True) # input_ids, token_type_ids, attention_mask 생성\n",
    "        input_ids = tokenized_inputs['input_ids'] # BERT 모델의 내부 voca에 있는 단어에 매칭된 인덱스 값\n",
    "        decode_tokens_list = []\n",
    "        for i in input_ids:\n",
    "            decode_tokens = self.tokenizer.convert_ids_to_tokens(i) # 인덱스 숫자 값을 다시 단어로 디코딩\n",
    "            decode_tokens_list.append(decode_tokens)\n",
    "\n",
    "        TOTAL_STORE, TOTAL_FOOD = [], []\n",
    "\n",
    "        for index, decode_tokens in enumerate(decode_tokens_list):\n",
    "            STORE, FOOD = [], []\n",
    "            for idx, decode_token in enumerate(decode_tokens):\n",
    "                if decode_token != '[CLS]' and decode_token != '[SEP]':\n",
    "                    if preds[index][idx] == 1:\n",
    "                        STORE.append(decode_token)\n",
    "                    elif preds[index][idx] == 2:\n",
    "                        FOOD.append(decode_token)\n",
    "\n",
    "            # 리스트를 하나의 문자열로 만들고 ##를 없애기\n",
    "            STORE_STR = ''\n",
    "            FOOD_STR = ''\n",
    "            for idx, token in enumerate(STORE):\n",
    "                if idx == 0:\n",
    "                    STORE_STR += token\n",
    "                else:\n",
    "                    if '##' in token:\n",
    "                        STORE_STR += token.replace('##', '')\n",
    "                    else:\n",
    "                        STORE_STR += ' ' + token\n",
    "\n",
    "            for idx, token in enumerate(FOOD):\n",
    "                if idx == 0:\n",
    "                    FOOD_STR += token\n",
    "                else:\n",
    "                    if '##' in token:\n",
    "                        FOOD_STR += token.replace('##', '')\n",
    "                    else:\n",
    "                        FOOD_STR += ' ' + token\n",
    "            \n",
    "            TOTAL_STORE.append(STORE_STR)\n",
    "            TOTAL_FOOD.append(FOOD_STR)\n",
    "\n",
    "            TOTAL_LIST = list(zip(TOTAL_STORE, TOTAL_FOOD))\n",
    "\n",
    "        return TOTAL_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e076f39e954532a3068af198841279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b37907db2d843fca66f2000e545ba81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efda48431316427d8268d58720b3471c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.021492358297109604, 'eval_runtime': 0.2265, 'eval_samples_per_second': 30.908, 'eval_steps_per_second': 30.908, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cdc75b0130419e918b58440e396a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006075933575630188, 'eval_runtime': 0.1989, 'eval_samples_per_second': 35.2, 'eval_steps_per_second': 35.2, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1f0ecaaa32471493d96b2c032013af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.008311931975185871, 'eval_runtime': 0.2053, 'eval_samples_per_second': 34.091, 'eval_steps_per_second': 34.091, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10ef3bc9ab1482dbafc7d1ecbdb08d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006871882360428572, 'eval_runtime': 0.2114, 'eval_samples_per_second': 33.115, 'eval_steps_per_second': 33.115, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1576c2bb8c144c1a5ffa5979a1e9d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.010608138516545296, 'eval_runtime': 0.2914, 'eval_samples_per_second': 24.02, 'eval_steps_per_second': 24.02, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62b9919f773472ba7b6bb773b0912d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012683370150625706, 'eval_runtime': 0.2834, 'eval_samples_per_second': 24.701, 'eval_steps_per_second': 24.701, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6aa71acbc7b4ef1ab778f8b7ce59fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012303105555474758, 'eval_runtime': 0.2882, 'eval_samples_per_second': 24.288, 'eval_steps_per_second': 24.288, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1101d3bb2ef84a08add14a8261315b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011857167817652225, 'eval_runtime': 0.2676, 'eval_samples_per_second': 26.157, 'eval_steps_per_second': 26.157, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53be7bfc33d9425ca785d67aa2135a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011697419919073582, 'eval_runtime': 0.202, 'eval_samples_per_second': 34.652, 'eval_steps_per_second': 34.652, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d74cb691a55401ea9480645e66f7cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01171762216836214, 'eval_runtime': 0.2369, 'eval_samples_per_second': 29.546, 'eval_steps_per_second': 29.546, 'epoch': 10.0}\n",
      "{'train_runtime': 84.5773, 'train_samples_per_second': 3.311, 'train_steps_per_second': 3.311, 'train_loss': 0.05086778572627476, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/bert-base'\n",
    "num_labels = 3\n",
    "label_names = ['그외', '매장명', '음식명']\n",
    "\n",
    "ner = NERTrainer(model_name, num_labels, label_names)\n",
    "new_dataset = ner.make_new_dataset('train.json')\n",
    "split_dataset = ner.split_training(new_dataset=new_dataset, test_size=0.2, lr=2e-5, batch_size=1, epoch=10, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383a6d43152e48b7a3c5283c933ff683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          그외       1.00      0.99      1.00       313\n",
      "         매장명       0.96      1.00      0.98        80\n",
      "         음식명       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           0.99       476\n",
      "   macro avg       0.99      1.00      0.99       476\n",
      "weighted avg       0.99      0.99      0.99       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = ner.evaluate(split_dataset)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ea45b0c2624569ace2aad0cecc3715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pred = ner.pred_object(split_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('도미노 피자', '디럭스 피자'),\n",
       " ('카페베네', '아메리카노'),\n",
       " ('서브웨이', '이탈리안 BMT 샌드위치'),\n",
       " ('김밥천국', '김밥 과 라면'),\n",
       " ('던킨도너츠', '글레이즈드 도넛'),\n",
       " ('빽다방', '카라멜 마키아토'),\n",
       " ('파리바게뜨', '크림빵')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_TEXT_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
