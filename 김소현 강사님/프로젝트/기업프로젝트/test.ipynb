{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # 파일로부터 데이터를 로드\n",
    "from transformers import AutoTokenizer # BERT 토크나이저를 자동으로 불러옴\n",
    "from transformers import AutoModelForTokenClassification # NER와 같은 토큰 분류 모델 불러옴\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset('json', data_files='train.json')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base') # 한국어에 최적화된 BERT 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataset):\n",
    "    tokens_list = []\n",
    "    ner_tags_list = []\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    for i in range(len(dataset)):\n",
    "        tokens = dataset[i]['tokens']\n",
    "        tokens_list.append(tokens)\n",
    "\n",
    "        ner_tags = dataset[i]['ner_tags']\n",
    "        ner_tags_list.append(ner_tags)\n",
    "\n",
    "        tokenized_inputs = tokenizer(tokens, truncation=True, is_split_into_words=True)\n",
    "        # truncation=True : 시퀀스가 모델의 최대 길이를 초과할 경우, 초과 부분을 잘라냄\n",
    "        input_ids = tokenized_inputs['input_ids']\n",
    "        input_ids_list.append(input_ids)\n",
    "\n",
    "        token_type_ids = tokenized_inputs['token_type_ids']\n",
    "        token_type_ids_list.append(token_type_ids)\n",
    "\n",
    "        attention_mask = tokenized_inputs['attention_mask']\n",
    "        attention_mask_list.append(attention_mask)\n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "        aligned_labels = [ner_tags[word_id] if word_id is not None else -100 for word_id in word_ids]\n",
    "        labels_list.append(aligned_labels)\n",
    "\n",
    "    data_dict = {\n",
    "        'tokens' : tokens_list,\n",
    "        'ner_tags' : ner_tags_list,\n",
    "        'input_ids' : input_ids_list,\n",
    "        'token_type_ids' : token_type_ids_list,\n",
    "        'attention_mask' : attention_mask_list,\n",
    "        'labels' : labels_list\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53369a290fc45b5b96293b334fe4eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce89febf1c6438e9e83cc47b8428a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12216877937316895, 'eval_runtime': 0.4383, 'eval_samples_per_second': 15.97, 'eval_steps_per_second': 15.97, 'epoch': 1.0}\n",
      "{'train_runtime': 14.8926, 'train_samples_per_second': 1.88, 'train_steps_per_second': 1.88, 'train_loss': 0.4194725581577846, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28, training_loss=0.4194725581577846, metrics={'train_runtime': 14.8926, 'train_samples_per_second': 1.88, 'train_steps_per_second': 1.88, 'total_flos': 179132794542.0, 'train_loss': 0.4194725581577846, 'epoch': 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "new_dataset = make_dataset(dataset['train'])\n",
    "split_dataset = new_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"klue/bert-base\", num_labels=3)\n",
    "# NER 작업을 위한 BERT 모델, num_labels=3 : 모델의 출력 레이어가 3개의 클래스를 분류\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # 모델 학습 결과를 저장할 경로\n",
    "    eval_strategy=\"epoch\",                  # 각 epoch 이후에 평가\n",
    "    learning_rate=2e-5,                     # 학습률\n",
    "    per_device_train_batch_size=1,          # 학습 시 배치 크기\n",
    "    per_device_eval_batch_size=1,           # 평가 시 배치 크기\n",
    "    num_train_epochs=1,                     # 총 학습 epoch 수\n",
    "    weight_decay=0.01,                      # 가중치 감소율 (과적합 방지)\n",
    "    # seed=1                                  # 시드값\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                            # 훈련할 모델\n",
    "    args=training_args,                     # 학습 파라미터\n",
    "    train_dataset=split_dataset[\"train\"],   # 훈련 데이터셋\n",
    "    eval_dataset=split_dataset[\"test\"],     # 평가 데이터셋\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./path_to_save_tokenizer/tokenizer_config.json',\n",
       " './path_to_save_tokenizer/special_tokens_map.json',\n",
       " './path_to_save_tokenizer/vocab.txt',\n",
       " './path_to_save_tokenizer/added_tokens.json',\n",
       " './path_to_save_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장\n",
    "model.save_pretrained(\"./path_to_save_model\")\n",
    "tokenizer.save_pretrained(\"./path_to_save_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abba1a8e60e1477b896d3305ec6e255c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         그 외       1.00      0.98      0.99       282\n",
      "         매장명       0.98      1.00      0.99        82\n",
      "         음식명       0.93      0.98      0.95        84\n",
      "\n",
      "    accuracy                           0.98       448\n",
      "   macro avg       0.97      0.98      0.98       448\n",
      "weighted avg       0.98      0.98      0.98       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측 수행 및 성능지표 확인\n",
    "predictions, label_ids, metrics = trainer.predict(split_dataset['train'])\n",
    "\n",
    "preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "pred_list, target_list = [], []\n",
    "for pred, label in zip(preds, label_ids):\n",
    "    pred_list.extend(pred)\n",
    "    target_list.extend([0 if i==-100 else i for i in label])\n",
    "\n",
    "report = classification_report(target_list, pred_list, target_names=['그 외', '매장명', '음식명'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa379341519d4905a7743e9aaab2ed42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, label_ids, metrics = trainer.predict(split_dataset['test'])\n",
    "preds = np.argmax(predictions, axis=2)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_object(dataset, index):\n",
    "    predictions, label_ids, metrics = trainer.predict(dataset)\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    tokens = dataset[index]['tokens']\n",
    "    tokenized_inputs = tokenizer(tokens, is_split_into_words=True, truncation=True) # input_ids, token_type_ids, attention_mask 생성\n",
    "    input_ids = tokenized_inputs['input_ids'] # BERT 모델의 내부 voca에 있는 단어에 매칭된 인덱스 값\n",
    "    decode_tokens = tokenizer.convert_ids_to_tokens(input_ids) # 인덱스 숫자 값을 다시 단어로 디코딩\n",
    "    # print(decode_tokens) # ex) ['[CLS]', '맥도날드', '에서', '빅', '##맥', '세트', '를', '구매', '##했', '##어요', '[SEP]']\n",
    "    # print(preds[index]) # ex) [0 1 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
    "\n",
    "    STORE = []\n",
    "    FOOD = []\n",
    "    for idx, decode_token in enumerate(decode_tokens):\n",
    "        if preds[index][idx] == 1:\n",
    "            STORE.append(decode_token)\n",
    "        elif preds[index][idx] == 2:\n",
    "            FOOD.append(decode_token)\n",
    "\n",
    "    # 리스트를 하나의 문자열로 만들고 ##를 없애기\n",
    "    STORE_STR = ''\n",
    "    FOOD_STR = ''\n",
    "    for idx, token in enumerate(STORE):\n",
    "        if idx == 0:\n",
    "            STORE_STR += token\n",
    "        else:\n",
    "            if '##' in token:\n",
    "                STORE_STR += token.replace('##', '')\n",
    "            else:\n",
    "                STORE_STR += ' ' + token\n",
    "\n",
    "    for idx, token in enumerate(FOOD):\n",
    "        if idx == 0:\n",
    "            FOOD_STR += token\n",
    "        else:\n",
    "            if '##' in token:\n",
    "                FOOD_STR += token.replace('##', '')\n",
    "            else:\n",
    "                FOOD_STR += ' ' + token\n",
    "    \n",
    "    return STORE_STR, FOOD_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b783ae292ab34d5380afec2fa285d656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('스타벅스', '카페라떼')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_object(split_dataset['test'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e810c4eb38c4d87ac99ac8911a858e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, label_ids, metrics = trainer.predict(split_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(predictions, axis=2)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(split_dataset['test']['tokens'], is_split_into_words=True, truncation=True)\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "decode_tokens_list = []\n",
    "for i in input_ids:\n",
    "    decode_tokens = tokenizer.convert_ids_to_tokens(i)\n",
    "    decode_tokens_list.append(decode_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '할리', '##스', '##커피', '에서', '카', '##푸', '##치', '##노', '를', '마셨', '##어요', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(decode_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_STORE = []\n",
    "TOTAL_FOOD = []\n",
    "for index, decode_tokens in enumerate(decode_tokens_list):\n",
    "    STORE, FOOD = [], []\n",
    "    for idx, decode_token in enumerate(decode_tokens):\n",
    "        if decode_token != '[CLS]' and decode_token != '[SEP]':\n",
    "            if preds[index][idx] == 1:\n",
    "                STORE.append(decode_token)\n",
    "            elif preds[index][idx] == 2:\n",
    "                FOOD.append(decode_token)\n",
    "\n",
    "    # 리스트를 하나의 문자열로 만들고 ##를 없애기\n",
    "    STORE_STR = ''\n",
    "    FOOD_STR = ''\n",
    "    for idx, token in enumerate(STORE):\n",
    "        if idx == 0:\n",
    "            STORE_STR += token\n",
    "        else:\n",
    "            if '##' in token:\n",
    "                STORE_STR += token.replace('##', '')\n",
    "            else:\n",
    "                STORE_STR += ' ' + token\n",
    "\n",
    "    for idx, token in enumerate(FOOD):\n",
    "        if idx == 0:\n",
    "            FOOD_STR += token\n",
    "        else:\n",
    "            if '##' in token:\n",
    "                FOOD_STR += token.replace('##', '')\n",
    "            else:\n",
    "                FOOD_STR += ' ' + token\n",
    "    \n",
    "    TOTAL_STORE.append(STORE_STR)\n",
    "    TOTAL_FOOD.append(FOOD_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(zip(TOTAL_STORE, TOTAL_FOOD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('할리스커피', '카푸치노'),\n",
       " ('김밥천국', '김밥 라면'),\n",
       " ('CU 편의점', '삼각김밥'),\n",
       " ('맥도날드', '빅맥 세트'),\n",
       " ('투썸플레이스', '뉴욕치즈케이크'),\n",
       " ('KFC', '핫윙'),\n",
       " ('크리스피크림 도넛', '오리지널 글레이즈드 도넛')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_TEXT_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
